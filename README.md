# â˜ï¸ Crypto Data Platform (GCP + Python + Terraform)

A serverless, event-driven data engineering platform that ingests, processes, and analyzes cryptocurrency market data. This project uses **Infrastructure as Code (IaC)** to deploy a scalable, self-healing architecture on Google Cloud Platform and includes a **Strategy Command Center** for visualization.

## ğŸ— Architecture

**Region:** `australia-southeast1` (Sydney)

The pipeline follows a "Medallion Architecture" (Bronze â†’ Silver â†’ Gold), where each stage automatically triggers the next, ending in a visualization layer.

1.  **Ingestion (Bronze Layer):**
    * **Source:** CoinGecko API.
    * **Compute:** Google Cloud Function (Python 3.10).
    * **Trigger:** Cloud Scheduler (Daily cron job).
    * **Storage:** Google Cloud Storage (Raw JSON).
    * **Function:** `bronze-ingest-func`

2.  **Processing (Silver Layer):**
    * **Trigger:** Event-Driven (Fires immediately when data lands in Bronze).
    * **Logic:** Pandas (Local) / DuckDB (Cloud) for cleaning and unpivoting.
    * **Transformation:** Handles missing fields, normalizes paths, and converts JSON to Columnar format (Parquet).
    * **Storage:** Google Cloud Storage (Parquet).
    * **Function:** `silver-process-func`

3.  **Analytics (Gold Layer):**
    * **Trigger:** Event-Driven (Fires immediately when data lands in Silver).
    * **Logic:** Aggregation & Window Functions.
        * Calculates **Avg/Min/Max Prices**.
        * Generates **Market Summary Reports**.
    * **Storage:** Google Cloud Storage (Aggregated CSV/Parquet).
    * **Function:** `gold-analyze-func`

4.  **Visualization (The Command Center):**
    * **Tool:** Streamlit (Python-based UI).
    * **Charts:** Plotly Interactive Graphs.
    * **Feature:** Connects directly to the Gold Bucket to visualize signals and price trends in real-time.

## ğŸ›  Tech Stack

* **Language:** Python 3.10
* **Infrastructure:** Terraform
* **Data Processing:** Pandas (Local), DuckDB (Cloud/OLAP)
* **Cloud:** Google Cloud Platform (Functions, Storage, Scheduler, IAM, Pub/Sub)
* **Visualization:** Streamlit, Plotly
* **Testing:** Pytest, Mocks (unittest.mock)
* **Data Format:** JSON (Raw) â†’ Parquet (Compressed) â†’ CSV (Analytics)

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ infra/                  # Terraform Infrastructure code
â”‚   â”œâ”€â”€ main.tf             # Resource definitions (Buckets, Functions, IAM)
â”‚   â”œâ”€â”€ variables.tf        # Input variable declarations
â”‚   â””â”€â”€ terraform.tfvars    # Configuration values (Region, IDs)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cloud_functions/    # Production-ready Cloud Functions
â”‚   â”‚   â”œâ”€â”€ bronze/         # Ingestion Logic (main.py)
â”‚   â”‚   â”œâ”€â”€ silver/         # Transformation Logic (Event-Driven)
â”‚   â”‚   â””â”€â”€ gold/           # Analytics & Signals Logic (Event-Driven)
â”‚   â”œâ”€â”€ pipeline/           # Local Data Pipeline Logic
â”‚   â”‚   â”œâ”€â”€ bronze/         # Local ingestion script (ingest.py)
â”‚   â”‚   â”œâ”€â”€ silver/         # Local cleaning script (clean.py)
â”‚   â”‚   â”œâ”€â”€ gold/           # Local analytics script (analyze.py)
â”‚   â”‚   â””â”€â”€ run_pipeline.py # Pipeline Orchestrator (Runs all layers)
â”‚   â””â”€â”€ dashboard.py        # Streamlit Strategy Dashboard
â”œâ”€â”€ tests/                  # Unit Test Suite
â”‚   â”œâ”€â”€ test_bronze.py      # Bronze Layer Tests (Mocked API)
â”‚   â””â”€â”€ test_silver.py      # Silver Layer Tests (Mocked GCS + Real DuckDB)
â”œâ”€â”€ data/                   # Local data storage (for testing)
â”‚   â”œâ”€â”€ bronze/             # Raw JSON files
â”‚   â”œâ”€â”€ silver/             # Cleaned Parquet files
â”‚   â””â”€â”€ gold/               # Final Aggregated CSVs
â””â”€â”€ README.md
```

## ğŸš€ Deployment Guide

### Prerequisites
- Google Cloud SDK (gcloud) installed and authenticated.
- Terraform installed.
- Python 3.10+ installed.

### 1. Infrastructure Setup
Navigate to the infrastructure folder and apply the Terraform configuration.

```bash
cd infra
terraform init
terraform plan
terraform apply
```

### 2. Manual Trigger (The "Domino Effect")
You only need to trigger the Bronze function. The rest of the pipeline is fully automated.
1. Trigger Bronze (Ingests API data).
2. Silver auto-starts (Cleans & Converts to Parquet).
3. Gold auto-starts (Calculates Financial Signals).

```bash
gcloud functions call bronze-ingest-func \
  --region=australia-southeast1 \
  --data='{}'
```

### 3. Verification & Visualization
To see the results in the Strategy Command Center:
```bash
# Authenticate locally to read from GCS
gcloud auth application-default login

# Launch the Dashboard
streamlit run src/dashboard.py
```

## ğŸ§ª Unit Testing
The project includes a robust test suite using `pytest` and `mocks` to verify logic without incurring cloud costs or hitting API rate limits.
```bash
# Set Python Path (Important for imports)
export PYTHONPATH=$PYTHONPATH:$(pwd)/src

# Run all tests with verbose output
python -m pytest tests/ -v
```

## ğŸ§ª Local Development
To run the logic locally without deploying to the cloud:
```bash
# Activate environment
source crypto-env/bin/activate

# Run the Orchestrator
python src/pipeline/run_pipeline.py
```
*Alternatively, you can run individual layers manually:*
```bash
python src/pipeline/bronze/ingest.py
python src/pipeline/silver/clean.py
python src/pipeline/gold/analyze.py
```

## ğŸ›¡ Security
- **Service Account**: Uses a dedicated `crypto-runner-sa` with restricted permissions (`storage.admin`).
- **Data Sovereignity**: All resources confined to `australia-southeast1`.
- **Secrets**: No API keys committed to the repository.
- **Circuit Breaker**: API ingestion includes error handling to halt pipeline on 4xx/5xx errors.
- **Schema Enforcement**: Strict typing in DuckDB prevents pipeline crashes from bad API data.
